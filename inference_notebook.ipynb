{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5041d1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:54:32.311007Z",
     "iopub.status.busy": "2023-06-14T16:54:32.310446Z",
     "iopub.status.idle": "2023-06-14T16:54:36.920528Z",
     "shell.execute_reply": "2023-06-14T16:54:36.919122Z"
    },
    "papermill": {
     "duration": 4.628791,
     "end_time": "2023-06-14T16:54:36.924059",
     "exception": false,
     "start_time": "2023-06-14T16:54:32.295268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099916b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:54:36.949716Z",
     "iopub.status.busy": "2023-06-14T16:54:36.949311Z",
     "iopub.status.idle": "2023-06-14T16:55:44.875384Z",
     "shell.execute_reply": "2023-06-14T16:55:44.874016Z"
    },
    "papermill": {
     "duration": 67.942023,
     "end_time": "2023-06-14T16:55:44.878300",
     "exception": false,
     "start_time": "2023-06-14T16:54:36.936277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/einops/einops-0.6.1-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.6.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/monai-packages/monai-1.1.0-202212191849-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from monai==1.1.0) (1.21.6)\r\n",
      "Requirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.7/site-packages (from monai==1.1.0) (1.13.0)\r\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (from monai==1.1.0) (0.6.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai==1.1.0) (4.4.0)\r\n",
      "Installing collected packages: monai\r\n",
      "Successfully installed monai-1.1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/einops/einops-0.6.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/monai-packages/monai-1.1.0-202212191849-py3-none-any.whl[\"einops\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb75315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:55:44.904852Z",
     "iopub.status.busy": "2023-06-14T16:55:44.903868Z",
     "iopub.status.idle": "2023-06-14T16:56:00.059229Z",
     "shell.execute_reply": "2023-06-14T16:56:00.057816Z"
    },
    "papermill": {
     "duration": 15.172294,
     "end_time": "2023-06-14T16:56:00.062639",
     "exception": false,
     "start_time": "2023-06-14T16:55:44.890345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n",
    "sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n",
    "sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n",
    "sys.path.append('/kaggle/input/unet3d/pytorch3dunet/pytorch3dunet')\n",
    "sys.path.append('/kaggle/input/unet3d/pytorch3dunet')\n",
    "sys.path.append('/kaggle/input/unet3d/')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from unet3d.model import get_model\n",
    "from unetr import UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eebbc05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.089367Z",
     "iopub.status.busy": "2023-06-14T16:56:00.088582Z",
     "iopub.status.idle": "2023-06-14T16:56:00.484114Z",
     "shell.execute_reply": "2023-06-14T16:56:00.482318Z"
    },
    "papermill": {
     "duration": 0.412184,
     "end_time": "2023-06-14T16:56:00.487302",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.075118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345fd4",
   "metadata": {
    "papermill": {
     "duration": 0.0123,
     "end_time": "2023-06-14T16:56:00.512292",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.499992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40f8bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.539570Z",
     "iopub.status.busy": "2023-06-14T16:56:00.539164Z",
     "iopub.status.idle": "2023-06-14T16:56:00.558047Z",
     "shell.execute_reply": "2023-06-14T16:56:00.556582Z"
    },
    "papermill": {
     "duration": 0.035624,
     "end_time": "2023-06-14T16:56:00.561034",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.525410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "\n",
    "    # comp_dir_path = './'\n",
    "    comp_dir_path = '/kaggle/input/'\n",
    "    comp_folder_name = 'vesuvius-challenge-ink-detection'\n",
    "    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n",
    "    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n",
    "    \n",
    "    exp_name = '3d_unet_subv2'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 1\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = '3d_unet_segformer'\n",
    "    backbone = 'None'\n",
    "#     backbone = 'se_resnext50_32x4d'\n",
    "\n",
    "    in_chans = 16\n",
    "    # ============== training cfg =============\n",
    "    size = 1024\n",
    "    tile_size = 1024\n",
    "    stride = tile_size // 4\n",
    "\n",
    "    batch_size = 3 # 32\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 15\n",
    "\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4 / warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    valid_id = 2\n",
    "\n",
    "    objective_cv = 'binary'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'maximize'  # maximize, 'minimize'\n",
    "    # metrics = 'dice_coef'\n",
    "\n",
    "    # ============== fixed =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'best'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 50\n",
    "    num_workers = 2\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07166ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.588218Z",
     "iopub.status.busy": "2023-06-14T16:56:00.587492Z",
     "iopub.status.idle": "2023-06-14T16:56:00.594333Z",
     "shell.execute_reply": "2023-06-14T16:56:00.593098Z"
    },
    "papermill": {
     "duration": 0.023514,
     "end_time": "2023-06-14T16:56:00.597217",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.573703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_DEBUG = False\n",
    "mode = 'train' if IS_DEBUG else 'test'\n",
    "TH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc296dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.625614Z",
     "iopub.status.busy": "2023-06-14T16:56:00.625212Z",
     "iopub.status.idle": "2023-06-14T16:56:00.697661Z",
     "shell.execute_reply": "2023-06-14T16:56:00.696220Z"
    },
    "papermill": {
     "duration": 0.090446,
     "end_time": "2023-06-14T16:56:00.700677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.610231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fcc0c",
   "metadata": {
    "papermill": {
     "duration": 0.012051,
     "end_time": "2023-06-14T16:56:00.725379",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.713328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d3181a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.752011Z",
     "iopub.status.busy": "2023-06-14T16:56:00.751036Z",
     "iopub.status.idle": "2023-06-14T16:56:00.759241Z",
     "shell.execute_reply": "2023-06-14T16:56:00.758068Z"
    },
    "papermill": {
     "duration": 0.024568,
     "end_time": "2023-06-14T16:56:00.762072",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.737504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba57cd",
   "metadata": {
    "papermill": {
     "duration": 0.012549,
     "end_time": "2023-06-14T16:56:00.787362",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.774813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9caa21a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.814200Z",
     "iopub.status.busy": "2023-06-14T16:56:00.813537Z",
     "iopub.status.idle": "2023-06-14T16:56:00.824872Z",
     "shell.execute_reply": "2023-06-14T16:56:00.823670Z"
    },
    "papermill": {
     "duration": 0.027887,
     "end_time": "2023-06-14T16:56:00.827677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.799790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "#     idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.in_chans // 2\n",
    "    end = mid + CFG.in_chans // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f54b80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.854151Z",
     "iopub.status.busy": "2023-06-14T16:56:00.853489Z",
     "iopub.status.idle": "2023-06-14T16:56:00.864132Z",
     "shell.execute_reply": "2023-06-14T16:56:00.863005Z"
    },
    "papermill": {
     "duration": 0.027028,
     "end_time": "2023-06-14T16:56:00.866823",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.839795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = np.array(images)\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.images[idx])\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image[None, :, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f476714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.892445Z",
     "iopub.status.busy": "2023-06-14T16:56:00.892092Z",
     "iopub.status.idle": "2023-06-14T16:56:00.903861Z",
     "shell.execute_reply": "2023-06-14T16:56:00.902731Z"
    },
    "papermill": {
     "duration": 0.027652,
     "end_time": "2023-06-14T16:56:00.906428",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.878776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.tile_size\n",
    "            x2 = x1 + CFG.tile_size\n",
    "            if test_images[y1:y2, x1:x2].max() != 0:\n",
    "                if not os.path.exists(f\"{x1}_{y1}_{x2}_{y2}.npy\"):\n",
    "                    np.save(f\"{x1}_{y1}_{x2}_{y2}.npy\", test_images[y1:y2, x1:x2])\n",
    "                test_images_list.append(f\"{x1}_{y1}_{x2}_{y2}.npy\")\n",
    "                xyxys.append((x1, y1, x2, y2))\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8db65",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2023-06-14T16:56:00.930782",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.918816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043fad1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.958358Z",
     "iopub.status.busy": "2023-06-14T16:56:00.957104Z",
     "iopub.status.idle": "2023-06-14T16:56:00.967635Z",
     "shell.execute_reply": "2023-06-14T16:56:00.966493Z"
    },
    "papermill": {
     "duration": 0.026857,
     "end_time": "2023-06-14T16:56:00.970244",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.943387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerModel, SegformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a31cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.996019Z",
     "iopub.status.busy": "2023-06-14T16:56:00.995557Z",
     "iopub.status.idle": "2023-06-14T16:56:01.007272Z",
     "shell.execute_reply": "2023-06-14T16:56:01.006215Z"
    },
    "papermill": {
     "duration": 0.027797,
     "end_time": "2023-06-14T16:56:01.009859",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.982062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b1_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 256,\n",
    "  \"depths\": [\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e6c1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.037314Z",
     "iopub.status.busy": "2023-06-14T16:56:01.036057Z",
     "iopub.status.idle": "2023-06-14T16:56:01.047110Z",
     "shell.execute_reply": "2023-06-14T16:56:01.045982Z"
    },
    "papermill": {
     "duration": 0.027332,
     "end_time": "2023-06-14T16:56:01.049571",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.022239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b2_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    6,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8372d5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.079941Z",
     "iopub.status.busy": "2023-06-14T16:56:01.078841Z",
     "iopub.status.idle": "2023-06-14T16:56:01.090213Z",
     "shell.execute_reply": "2023-06-14T16:56:01.089169Z"
    },
    "papermill": {
     "duration": 0.027397,
     "end_time": "2023-06-14T16:56:01.092770",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.065373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b4_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    8,\n",
    "    27,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "108de382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.120714Z",
     "iopub.status.busy": "2023-06-14T16:56:01.120345Z",
     "iopub.status.idle": "2023-06-14T16:56:01.131061Z",
     "shell.execute_reply": "2023-06-14T16:56:01.129732Z"
    },
    "papermill": {
     "duration": 0.027203,
     "end_time": "2023-06-14T16:56:01.133534",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.106331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b5_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b90b4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.160321Z",
     "iopub.status.busy": "2023-06-14T16:56:01.159911Z",
     "iopub.status.idle": "2023-06-14T16:56:01.181986Z",
     "shell.execute_reply": "2023-06-14T16:56:01.180972Z"
    },
    "papermill": {
     "duration": 0.038961,
     "end_time": "2023-06-14T16:56:01.184541",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.145580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":32})\n",
    "cnn_3d_more_filters_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":64})\n",
    "\n",
    "unet_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 3,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561503be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.211158Z",
     "iopub.status.busy": "2023-06-14T16:56:01.210122Z",
     "iopub.status.idle": "2023-06-14T16:56:01.228234Z",
     "shell.execute_reply": "2023-06-14T16:56:01.226955Z"
    },
    "papermill": {
     "duration": 0.033891,
     "end_time": "2023-06-14T16:56:01.230741",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.196850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_3d_jumbo_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})\n",
    "\n",
    "unetr_multiclass_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95594b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.258876Z",
     "iopub.status.busy": "2023-06-14T16:56:01.258479Z",
     "iopub.status.idle": "2023-06-14T16:56:01.307642Z",
     "shell.execute_reply": "2023-06-14T16:56:01.306544Z"
    },
    "papermill": {
     "duration": 0.066905,
     "end_time": "2023-06-14T16:56:01.310516",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.243611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unetr import UNETR\n",
    "class UNETR_Segformer(nn.Module):\n",
    "    def __init__(self, cfg, dropout = .2):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.encoder = UNETR(\n",
    "            in_channels=1,\n",
    "            out_channels=32,\n",
    "            img_size=(16, self.cfg.size, self.cfg.size),\n",
    "            conv_block=True\n",
    "        )\n",
    "        self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(\n",
    "            1, 1, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(\n",
    "            1, 1, kernel_size=(4, 4), stride=2, padding=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        output = self.encoder(image).max(axis=2)[0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output\n",
    "class UNETR_SegformerMC(nn.Module):\n",
    "    def __init__(self, cfg, dropout = .2):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.encoder = UNETR(\n",
    "            in_channels=1,\n",
    "            out_channels=32,\n",
    "            img_size=(16, self.cfg.size, self.cfg.size),\n",
    "#             conv_block=True\n",
    "        )\n",
    "        self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(\n",
    "            3, 3, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(\n",
    "            3, 3, kernel_size=(4, 4), stride=2, padding=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        output = self.encoder(image).max(axis=2)[0]\n",
    "        output = self.dropout(output)\n",
    "        output = self.encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output[:, 2:, :, :]\n",
    "    \n",
    "class cnn3d_segformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.conv3d_1 = nn.Conv3d(1, 4, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_2 = nn.Conv3d(4, 8, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_3 = nn.Conv3d(8, 16, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_4 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "\n",
    "        self.xy_encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        output = self.conv3d_1(image)\n",
    "        output = self.conv3d_2(output)\n",
    "        output = self.conv3d_3(output)\n",
    "        output = self.conv3d_4(output).max(axis = 2)[0]\n",
    "        output = self.xy_encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output\n",
    "    \n",
    "class cnn3d_segformer_more_filters(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.conv3d_1 = nn.Conv3d(1, 4, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_2 = nn.Conv3d(4, 8, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_3 = nn.Conv3d(8, 16, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "        self.conv3d_4 = nn.Conv3d(16, 64, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "\n",
    "        self.xy_encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        output = self.conv3d_1(image)\n",
    "        output = self.conv3d_2(output)\n",
    "        output = self.conv3d_3(output)\n",
    "        output = self.conv3d_4(output).max(axis = 2)[0]\n",
    "        output = self.xy_encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output\n",
    "    \n",
    "class unet3d_segformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = get_model({\"name\":\"UNet3D\", \"in_channels\":1, \"out_channels\":16, \"f_maps\":8, \"num_groups\":4, \"is_segmentation\":False})\n",
    "        self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "    def forward(self, image):\n",
    "        output = self.model(image).max(axis = 2)[0]\n",
    "        output = self.encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output\n",
    "    \n",
    "class unet3d_segformer_jumbo(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = get_model({\"name\":\"UNet3D\", \"in_channels\":1, \"out_channels\":32, \"f_maps\":8, \"num_groups\":4, \"is_segmentation\":False})\n",
    "        self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "        self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "    def forward(self, image):\n",
    "        output = self.model(image).max(axis = 2)[0]\n",
    "        output = self.encoder_2d(output).logits\n",
    "        output = self.upscaler1(output)\n",
    "        output = self.upscaler2(output)\n",
    "        return output\n",
    "    \n",
    "def build_model(cfg, model_arch = None):\n",
    "    print('model_name', cfg.model_name)\n",
    "    if model_arch == \"cnn3d\":\n",
    "        model = cnn3d_segformer(cfg)\n",
    "    if model_arch == \"cnn3d_more_filters\":\n",
    "        model = cnn3d_segformer_more_filters(cfg)\n",
    "    if model_arch == \"unet3d\":\n",
    "        model = unet3d_segformer(cfg)\n",
    "    if model_arch == \"unet3d_jumbo\":\n",
    "        model = unet3d_segformer_jumbo(cfg)\n",
    "    if model_arch == \"unetr\":\n",
    "        model = UNETR_Segformer(cfg)\n",
    "    if model_arch == \"unetr_mc\":\n",
    "        model = UNETR_SegformerMC(cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f168c01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.337829Z",
     "iopub.status.busy": "2023-06-14T16:56:01.336745Z",
     "iopub.status.idle": "2023-06-14T16:56:01.353471Z",
     "shell.execute_reply": "2023-06-14T16:56:01.352415Z"
    },
    "papermill": {
     "duration": 0.033329,
     "end_time": "2023-06-14T16:56:01.356342",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.323013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, use_tta=False):\n",
    "        self.models = []\n",
    "        self.use_tta = use_tta\n",
    "    def tta_infer(self, model:nn.Module, x):\n",
    "        #x.shape=(batch,c,h,w)\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=[model(single_x) for single_x in x]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=x.reshape(4,shape[0],*shape[3:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        if self.use_tta:\n",
    "            outputs = [self.tta_infer(model, x).to('cpu').numpy()\n",
    "                   for model in self.models]\n",
    "        else:\n",
    "            outputs = [model(x).mean(axis = 1).to('cpu').numpy()\n",
    "                       for model in self.models]\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "def build_ensemble_model(model_path, model_arch):\n",
    "    model = EnsembleModel(use_tta = True)\n",
    "    _model = build_model(CFG, model_arch)\n",
    "    _model.to(device)\n",
    "    state = torch.load(model_path)\n",
    "    try:\n",
    "        _model.load_state_dict(state)\n",
    "    except:\n",
    "        _model = nn.DataParallel(_model)\n",
    "        _model.load_state_dict(state)\n",
    "    _model.eval()\n",
    "\n",
    "    model.add_model(_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e781a384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.382994Z",
     "iopub.status.busy": "2023-06-14T16:56:01.382575Z",
     "iopub.status.idle": "2023-06-14T16:56:01.407937Z",
     "shell.execute_reply": "2023-06-14T16:56:01.406663Z"
    },
    "papermill": {
     "duration": 0.041571,
     "end_time": "2023-06-14T16:56:01.410668",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.369097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == 'test':\n",
    "    fragment_ids = sorted(os.listdir(CFG.comp_dataset_path + mode))\n",
    "else:\n",
    "    fragment_ids = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7bf600f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.437997Z",
     "iopub.status.busy": "2023-06-14T16:56:01.437565Z",
     "iopub.status.idle": "2023-06-14T16:56:01.455070Z",
     "shell.execute_reply": "2023-06-14T16:56:01.453650Z"
    },
    "papermill": {
     "duration": 0.035023,
     "end_time": "2023-06-14T16:56:01.457896",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.422873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tuples = [\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_3dcnn_segformer_best.pth\", \"segformer_config\": cnn_3d_config, \"score\": .75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_1024_3d_unet_segformer_final_all_train.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_512_3d_unet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_full_train_3dcnn_segformer_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_swa_slow_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\".74},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_3dunet_segformer_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_all_train_3dunet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_10_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_15_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_20_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_25_final.pth\", \"segformer_config\": cnn_3d_config},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 5,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b1_3dcnn_segformer_b1_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b1_config, \"score\":.71},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b2_3dcnn_segformer_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config, \"score\":.68},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_b4_3dcnn_segformer_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_bigsegformer_3dcnn_bigsegformer_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_10_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_30_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\": .74},\n",
    "    {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_all_train_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_all_train_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_512_b2_all_train_3dcnn_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config},\n",
    "    # ran at wrong resolution. Scored .73\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa_all_train.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.79},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_new_label_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_5_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\": .69},\n",
    "#     {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_1245_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config},\n",
    "#     {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_512_b5_unet_final_4Ryan.pth\", \"segformer_config\": unetr_multiclass_config, \"score\":.77},\n",
    "    {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_888_final_swa_all_train_long.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.82},\n",
    "    {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_NOVALIDATION_512_b5_unet_final_swa_all_train.pth\", \"segformer_config\": unetr_multiclass_config},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baedd7",
   "metadata": {
    "papermill": {
     "duration": 0.012363,
     "end_time": "2023-06-14T16:56:01.482977",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.470614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8128a58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.509903Z",
     "iopub.status.busy": "2023-06-14T16:56:01.509503Z",
     "iopub.status.idle": "2023-06-14T16:56:01.517432Z",
     "shell.execute_reply": "2023-06-14T16:56:01.516223Z"
    },
    "papermill": {
     "duration": 0.02438,
     "end_time": "2023-06-14T16:56:01.520106",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.495726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size = 20000):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros_like(probability, np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd8a630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.548882Z",
     "iopub.status.busy": "2023-06-14T16:56:01.547478Z",
     "iopub.status.idle": "2023-06-14T16:56:01.553530Z",
     "shell.execute_reply": "2023-06-14T16:56:01.552414Z"
    },
    "papermill": {
     "duration": 0.022776,
     "end_time": "2023-06-14T16:56:01.555928",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.533152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec000a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.582573Z",
     "iopub.status.busy": "2023-06-14T16:56:01.582187Z",
     "iopub.status.idle": "2023-06-14T16:56:01.607060Z",
     "shell.execute_reply": "2023-06-14T16:56:01.605799Z"
    },
    "papermill": {
     "duration": 0.041126,
     "end_time": "2023-06-14T16:56:01.609633",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.568507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    for fragment_id in fragment_ids:\n",
    "        mask_preds = None\n",
    "        last_res = None\n",
    "        for model_config in model_tuples:\n",
    "            mask_pred = None\n",
    "            mask_count = None\n",
    "            if last_res != model_config[\"size\"]:\n",
    "                for file in glob.glob(\"*.npy\"):\n",
    "                    os.remove(file)\n",
    "                last_res = model_config[\"size\"]\n",
    "            CFG.tile_size = model_config[\"tile_size\"]\n",
    "            CFG.size = model_config[\"size\"]\n",
    "            CFG.batch_size = model_config[\"batch_size\"]\n",
    "            CFG.stride = CFG.tile_size // 4\n",
    "            CFG.valid_aug_list = [\n",
    "                A.Resize(CFG.size, CFG.size),\n",
    "                A.Normalize(\n",
    "                    mean= [0] * CFG.in_chans,\n",
    "                    std= [1] * CFG.in_chans\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "            CFG.segformer_config = model_config[\"segformer_config\"]\n",
    "            model = build_ensemble_model(model_config[\"weight_path\"], model_config[\"model_arch\"])\n",
    "            test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "\n",
    "            binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/mask.png\", 0)\n",
    "            binary_mask = (binary_mask / 255).astype(int)\n",
    "\n",
    "            ori_h = binary_mask.shape[0]\n",
    "            ori_w = binary_mask.shape[1]\n",
    "            # mask = mask / 255\n",
    "\n",
    "            pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n",
    "            pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n",
    "\n",
    "            binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "            if mask_pred is None:\n",
    "                mask_pred = np.zeros(binary_mask.shape)\n",
    "                mask_count = np.zeros(binary_mask.shape)\n",
    "\n",
    "            for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "                images = images.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                with autocast():            \n",
    "                    with torch.no_grad():\n",
    "                        y_preds = model(images)\n",
    "\n",
    "                start_idx = step*CFG.batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "                    mask_pred[y1:y2, x1:x2] += y_preds[i]\n",
    "                    mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n",
    "            del test_loader\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "            mask_count = mask_count[:ori_h, :ori_w]\n",
    "            binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "\n",
    "            print(f'mask_count_min: {mask_count.min()}')\n",
    "            mask_pred = mask_pred/mask_count\n",
    "            mask_pred = torch.sigmoid(torch.tensor(mask_pred)).numpy()\n",
    "            if mask_preds is None:\n",
    "                mask_preds = mask_pred/len(model_tuples)\n",
    "            else:\n",
    "                mask_preds += mask_pred/len(model_tuples)\n",
    "\n",
    "        mask_pred = (mask_preds >= TH).astype(int)\n",
    "        mask_pred *= binary_mask\n",
    "        mask_pred = post_process(mask_pred.astype(float), TH, 10000).astype(int)\n",
    "        plt.imshow(mask_pred)\n",
    "        inklabels_rle = rle(mask_pred)\n",
    "        results.append((fragment_id, inklabels_rle))\n",
    "        del mask_pred, mask_count\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for file in glob.glob(\"*.npy\"):\n",
    "            os.remove(file)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142d46b",
   "metadata": {
    "papermill": {
     "duration": 0.012421,
     "end_time": "2023-06-14T16:56:01.634839",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.622418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "503e5f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.662053Z",
     "iopub.status.busy": "2023-06-14T16:56:01.661624Z",
     "iopub.status.idle": "2023-06-14T16:56:01.673327Z",
     "shell.execute_reply": "2023-06-14T16:56:01.672237Z"
    },
    "papermill": {
     "duration": 0.02818,
     "end_time": "2023-06-14T16:56:01.676031",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.647851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(results, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c369850b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.702335Z",
     "iopub.status.busy": "2023-06-14T16:56:01.701688Z",
     "iopub.status.idle": "2023-06-14T16:56:01.718810Z",
     "shell.execute_reply": "2023-06-14T16:56:01.717681Z"
    },
    "papermill": {
     "duration": 0.033011,
     "end_time": "2023-06-14T16:56:01.721272",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.688261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Predicted]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e3da14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.748670Z",
     "iopub.status.busy": "2023-06-14T16:56:01.747527Z",
     "iopub.status.idle": "2023-06-14T16:56:01.778950Z",
     "shell.execute_reply": "2023-06-14T16:56:01.777668Z"
    },
    "papermill": {
     "duration": 0.04794,
     "end_time": "2023-06-14T16:56:01.781769",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.733829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(CFG.comp_dataset_path + 'sample_submission.csv')\n",
    "sample_sub = pd.merge(sample_sub[['Id']], sub, on='Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1cb6bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.808627Z",
     "iopub.status.busy": "2023-06-14T16:56:01.808233Z",
     "iopub.status.idle": "2023-06-14T16:56:01.819978Z",
     "shell.execute_reply": "2023-06-14T16:56:01.818791Z"
    },
    "papermill": {
     "duration": 0.028162,
     "end_time": "2023-06-14T16:56:01.822408",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.794246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id Predicted\n",
       "0  a       NaN\n",
       "1  b       NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "971b5fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.849823Z",
     "iopub.status.busy": "2023-06-14T16:56:01.848694Z",
     "iopub.status.idle": "2023-06-14T16:56:01.857974Z",
     "shell.execute_reply": "2023-06-14T16:56:01.856485Z"
    },
    "papermill": {
     "duration": 0.025832,
     "end_time": "2023-06-14T16:56:01.860663",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.834831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 104.261062,
   "end_time": "2023-06-14T16:56:04.777148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T16:54:20.516086",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
