{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3090fc-da18-4a47-ba96-3e967557d165",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "\n",
    "Use this notebook to run inference using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2fc9a3-6ebb-487e-a7ce-b9724fec373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.0.74-cp37-abi3-macosx_11_0_arm64.whl (33.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from opencv-python) (1.25.0)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.0.74\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfdd6a7-d23a-4cb2-94b8-d8c90d87bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Using cached pathtools-0.1.2-py3-none-any.whl\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.2-cp310-cp310-macosx_10_9_universal2.whl (16 kB)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 protobuf-4.23.4 sentry-sdk-1.28.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045cb44f-68eb-43f7-b3c2-dc2e2cc22f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai\n",
      "  Downloading monai-1.2.0-202306081546-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from monai) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from monai) (1.25.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from torch>=1.9->monai) (4.7.0)\n",
      "Installing collected packages: monai\n",
      "Successfully installed monai-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9755d90-5b33-4d3a-97ae-bd89f2ede704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5041d1d0",
   "metadata": {
    "papermill": {
     "duration": 4.628791,
     "end_time": "2023-06-14T16:54:36.924059",
     "exception": false,
     "start_time": "2023-06-14T16:54:32.295268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099916b5",
   "metadata": {
    "papermill": {
     "duration": 67.942023,
     "end_time": "2023-06-14T16:55:44.878300",
     "exception": false,
     "start_time": "2023-06-14T16:54:36.936277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/einops/einops-0.6.1-py3-none-any.whl\n",
    "# !pip install /kaggle/input/monai-packages/monai-1.1.0-202212191849-py3-none-any.whl[\"einops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7de1c-cc29-4108-aa2e-392c83bc33a5",
   "metadata": {},
   "source": [
    "To get the folders necessary for the path appends below, check the README for instructions.\n",
    "\n",
    "See the part that says  \n",
    "`!cd /root && git clone git@github.com:wolny/pytorch-3dunet.git && cd pytorch-3dunet && pip install -e .` You just need to grab the `get_model` function from this repo and grab the model directly. It was slightly annoying for us with imports, you might have to do some `sys.path.append` stuff to get the imports working.\n",
    "\n",
    "In this inference notebook we originally manually added the paths to `segmentation-models-pytorch`, `timm-pytorch-image-models`, `efficientnet-pytorch`, and `pretrainedmodels`, as well as the 3d-unet. On the actual notebook we submitted, we put the models on the github links above into the kaggle datasets below:\n",
    "\n",
    "https://www.kaggle.com/datasets/ryches/einops  \n",
    "https://www.kaggle.com/datasets/ryches/3d-unet  \n",
    "https://www.kaggle.com/datasets/ryches/unet3d  \n",
    "\n",
    "Here are the links to the other repos. You likely won't have to install these manually as they're included in `requirements.txt`. We're including links anyway in case you have to go that route. Make sure to edit the `sys.path.append` calls below to include if you're grabbing these manually (if you can import as-is, feel free to comment out the `sys.path.append`s)\n",
    "\n",
    "https://github.com/qubvel/segmentation_models.pytorch  \n",
    "https://timm.fast.ai/  \n",
    "https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "The `pretrainedmodels` folder contains the weights we created during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6eb75315",
   "metadata": {
    "papermill": {
     "duration": 15.172294,
     "end_time": "2023-06-14T16:56:00.062639",
     "exception": false,
     "start_time": "2023-06-14T16:55:44.890345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unet3d.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/unet3d/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegmentation_models_pytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmpx\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munet3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munetr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNETR\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unet3d.model'"
     ]
    }
   ],
   "source": [
    "sys.path.append('externals')\n",
    "sys.path.append('externals/pytorch3dunet')\n",
    "\n",
    "\n",
    "sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n",
    "sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n",
    "sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n",
    "sys.path.append('/kaggle/input/unet3d/pytorch3dunet/pytorch3dunet')\n",
    "sys.path.append('/kaggle/input/unet3d/pytorch3dunet')\n",
    "sys.path.append('/kaggle/input/unet3d/')\n",
    "\n",
    "import segmentation_models_pytorch as smpx\n",
    "from unet3d.model import get_model\n",
    "from unetr import UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9eebbc05",
   "metadata": {
    "papermill": {
     "duration": 0.412184,
     "end_time": "2023-06-14T16:56:00.487302",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.075118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345fd4",
   "metadata": {
    "papermill": {
     "duration": 0.0123,
     "end_time": "2023-06-14T16:56:00.512292",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.499992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a40f8bc1",
   "metadata": {
    "papermill": {
     "duration": 0.035624,
     "end_time": "2023-06-14T16:56:00.561034",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.525410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "\n",
    "    # comp_dir_path = './'\n",
    "    comp_dir_path = '/kaggle/input/'\n",
    "    comp_folder_name = 'vesuvius-challenge-ink-detection'\n",
    "    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n",
    "    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n",
    "    \n",
    "    exp_name = '3d_unet_subv2'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 1\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = '3d_unet_segformer'\n",
    "    backbone = 'None'\n",
    "#     backbone = 'se_resnext50_32x4d'\n",
    "\n",
    "    in_chans = 16\n",
    "    # ============== training cfg =============\n",
    "    size = 1024\n",
    "    tile_size = 1024\n",
    "    stride = tile_size // 4\n",
    "\n",
    "    batch_size = 3 # 32\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 15\n",
    "\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4 / warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    valid_id = 2\n",
    "\n",
    "    objective_cv = 'binary'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'maximize'  # maximize, 'minimize'\n",
    "    # metrics = 'dice_coef'\n",
    "\n",
    "    # ============== fixed =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'best'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 50\n",
    "    num_workers = 2\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07166ff2",
   "metadata": {
    "papermill": {
     "duration": 0.023514,
     "end_time": "2023-06-14T16:56:00.597217",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.573703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_DEBUG = False\n",
    "mode = 'train' if IS_DEBUG else 'test'\n",
    "TH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fc296dd",
   "metadata": {
    "papermill": {
     "duration": 0.090446,
     "end_time": "2023-06-14T16:56:00.700677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.610231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fcc0c",
   "metadata": {
    "papermill": {
     "duration": 0.012051,
     "end_time": "2023-06-14T16:56:00.725379",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.713328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8d3181a",
   "metadata": {
    "papermill": {
     "duration": 0.024568,
     "end_time": "2023-06-14T16:56:00.762072",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.737504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba57cd",
   "metadata": {
    "papermill": {
     "duration": 0.012549,
     "end_time": "2023-06-14T16:56:00.787362",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.774813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9caa21a8",
   "metadata": {
    "papermill": {
     "duration": 0.027887,
     "end_time": "2023-06-14T16:56:00.827677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.799790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "#     idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.in_chans // 2\n",
    "    end = mid + CFG.in_chans // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f54b80e",
   "metadata": {
    "papermill": {
     "duration": 0.027028,
     "end_time": "2023-06-14T16:56:00.866823",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.839795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = np.array(images)\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.images[idx])\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image[None, :, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f476714",
   "metadata": {
    "papermill": {
     "duration": 0.027652,
     "end_time": "2023-06-14T16:56:00.906428",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.878776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.tile_size\n",
    "            x2 = x1 + CFG.tile_size\n",
    "            if test_images[y1:y2, x1:x2].max() != 0:\n",
    "                if not os.path.exists(f\"{x1}_{y1}_{x2}_{y2}.npy\"):\n",
    "                    np.save(f\"{x1}_{y1}_{x2}_{y2}.npy\", test_images[y1:y2, x1:x2])\n",
    "                test_images_list.append(f\"{x1}_{y1}_{x2}_{y2}.npy\")\n",
    "                xyxys.append((x1, y1, x2, y2))\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8db65",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2023-06-14T16:56:00.930782",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.918816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26ff61-0019-4504-bbf0-3c270908e9c1",
   "metadata": {},
   "source": [
    "Below we define the configs for different models we used in the eventual ensemble. We changed which models we actually included depending on the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "043fad1b",
   "metadata": {
    "papermill": {
     "duration": 0.026857,
     "end_time": "2023-06-14T16:56:00.970244",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.943387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerModel, SegformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a31cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:00.996019Z",
     "iopub.status.busy": "2023-06-14T16:56:00.995557Z",
     "iopub.status.idle": "2023-06-14T16:56:01.007272Z",
     "shell.execute_reply": "2023-06-14T16:56:01.006215Z"
    },
    "papermill": {
     "duration": 0.027797,
     "end_time": "2023-06-14T16:56:01.009859",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.982062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b1_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 256,\n",
    "  \"depths\": [\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e6c1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.037314Z",
     "iopub.status.busy": "2023-06-14T16:56:01.036057Z",
     "iopub.status.idle": "2023-06-14T16:56:01.047110Z",
     "shell.execute_reply": "2023-06-14T16:56:01.045982Z"
    },
    "papermill": {
     "duration": 0.027332,
     "end_time": "2023-06-14T16:56:01.049571",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.022239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b2_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    6,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8372d5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.079941Z",
     "iopub.status.busy": "2023-06-14T16:56:01.078841Z",
     "iopub.status.idle": "2023-06-14T16:56:01.090213Z",
     "shell.execute_reply": "2023-06-14T16:56:01.089169Z"
    },
    "papermill": {
     "duration": 0.027397,
     "end_time": "2023-06-14T16:56:01.092770",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.065373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b4_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    8,\n",
    "    27,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "108de382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.120714Z",
     "iopub.status.busy": "2023-06-14T16:56:01.120345Z",
     "iopub.status.idle": "2023-06-14T16:56:01.131061Z",
     "shell.execute_reply": "2023-06-14T16:56:01.129732Z"
    },
    "papermill": {
     "duration": 0.027203,
     "end_time": "2023-06-14T16:56:01.133534",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.106331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b5_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b90b4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.160321Z",
     "iopub.status.busy": "2023-06-14T16:56:01.159911Z",
     "iopub.status.idle": "2023-06-14T16:56:01.181986Z",
     "shell.execute_reply": "2023-06-14T16:56:01.180972Z"
    },
    "papermill": {
     "duration": 0.038961,
     "end_time": "2023-06-14T16:56:01.184541",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.145580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":32})\n",
    "cnn_3d_more_filters_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":64})\n",
    "\n",
    "unet_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 3,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561503be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.211158Z",
     "iopub.status.busy": "2023-06-14T16:56:01.210122Z",
     "iopub.status.idle": "2023-06-14T16:56:01.228234Z",
     "shell.execute_reply": "2023-06-14T16:56:01.226955Z"
    },
    "papermill": {
     "duration": 0.033891,
     "end_time": "2023-06-14T16:56:01.230741",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.196850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_3d_jumbo_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})\n",
    "\n",
    "unetr_multiclass_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95594b7c",
   "metadata": {
    "papermill": {
     "duration": 0.066905,
     "end_time": "2023-06-14T16:56:01.310516",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.243611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3dunet.unet3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 133\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munetr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNETR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# class UNETR_Segformer(nn.Module):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     def __init__(self, cfg, dropout = .2):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#         super().__init__()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#         output = self.upscaler2(output)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#         return output\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cnn3d_segformer, cnn3d_segformer_more_filters, unet3d_segformer, unet3d_segformer_jumbo, UNETR_Segformer, UNETR_SegformerMC\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(cfg, model_arch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/ink/Vesuvius-InkDetection/externals/models.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch3dunet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munet3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SegformerForSemanticSegmentation, SegformerModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3dunet.unet3d'"
     ]
    }
   ],
   "source": [
    "from unetr import UNETR\n",
    "\n",
    "# class UNETR_Segformer(nn.Module):\n",
    "#     def __init__(self, cfg, dropout = .2):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.dropout = nn.Dropout2d(dropout)\n",
    "#         self.encoder = UNETR(\n",
    "#             in_channels=1,\n",
    "#             out_channels=32,\n",
    "#             img_size=(16, self.cfg.size, self.cfg.size),\n",
    "#             conv_block=True\n",
    "#         )\n",
    "#         self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(\n",
    "#             1, 1, kernel_size=(4, 4), stride=2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(\n",
    "#             1, 1, kernel_size=(4, 4), stride=2, padding=1)\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         output = self.encoder(image).max(axis=2)[0]\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output\n",
    "        \n",
    "# class UNETR_SegformerMC(nn.Module):\n",
    "#     def __init__(self, cfg, dropout = .2):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "#         self.dropout = nn.Dropout2d(dropout)\n",
    "#         self.encoder = UNETR(\n",
    "#             in_channels=1,\n",
    "#             out_channels=32,\n",
    "#             img_size=(16, self.cfg.size, self.cfg.size),\n",
    "# #             conv_block=True\n",
    "#         )\n",
    "#         self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(\n",
    "#             3, 3, kernel_size=(4, 4), stride=2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(\n",
    "#             3, 3, kernel_size=(4, 4), stride=2, padding=1)\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         output = self.encoder(image).max(axis=2)[0]\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output[:, 2:, :, :]\n",
    "    \n",
    "# class cnn3d_segformer(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.conv3d_1 = nn.Conv3d(1, 4, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_2 = nn.Conv3d(4, 8, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_3 = nn.Conv3d(8, 16, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_4 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "\n",
    "#         self.xy_encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        \n",
    "#     def forward(self, image):\n",
    "#         output = self.conv3d_1(image)\n",
    "#         output = self.conv3d_2(output)\n",
    "#         output = self.conv3d_3(output)\n",
    "#         output = self.conv3d_4(output).max(axis = 2)[0]\n",
    "#         output = self.xy_encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output\n",
    "    \n",
    "# class cnn3d_segformer_more_filters(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.conv3d_1 = nn.Conv3d(1, 4, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_2 = nn.Conv3d(4, 8, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_3 = nn.Conv3d(8, 16, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "#         self.conv3d_4 = nn.Conv3d(16, 64, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1))\n",
    "\n",
    "#         self.xy_encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "        \n",
    "#     def forward(self, image):\n",
    "#         output = self.conv3d_1(image)\n",
    "#         output = self.conv3d_2(output)\n",
    "#         output = self.conv3d_3(output)\n",
    "#         output = self.conv3d_4(output).max(axis = 2)[0]\n",
    "#         output = self.xy_encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output\n",
    "    \n",
    "# class unet3d_segformer(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.model = get_model({\"name\":\"UNet3D\", \"in_channels\":1, \"out_channels\":16, \"f_maps\":8, \"num_groups\":4, \"is_segmentation\":False})\n",
    "#         self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#     def forward(self, image):\n",
    "#         output = self.model(image).max(axis = 2)[0]\n",
    "#         output = self.encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output\n",
    "    \n",
    "# class unet3d_segformer_jumbo(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.model = get_model({\"name\":\"UNet3D\", \"in_channels\":1, \"out_channels\":32, \"f_maps\":8, \"num_groups\":4, \"is_segmentation\":False})\n",
    "#         self.encoder_2d = SegformerForSemanticSegmentation(self.cfg.segformer_config)\n",
    "#         self.upscaler1 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#         self.upscaler2 = nn.ConvTranspose2d(1, 1, kernel_size=(4, 4), stride = 2, padding=1)\n",
    "#     def forward(self, image):\n",
    "#         output = self.model(image).max(axis = 2)[0]\n",
    "#         output = self.encoder_2d(output).logits\n",
    "#         output = self.upscaler1(output)\n",
    "#         output = self.upscaler2(output)\n",
    "#         return output\n",
    "\n",
    "from externals.models import cnn3d_segformer, cnn3d_segformer_more_filters, unet3d_segformer, unet3d_segformer_jumbo, UNETR_Segformer, UNETR_SegformerMC\n",
    "\n",
    "def build_model(cfg, model_arch = None):\n",
    "    print('model_name', cfg.model_name)\n",
    "    if model_arch == \"cnn3d\":\n",
    "        model = cnn3d_segformer(cfg)\n",
    "    if model_arch == \"cnn3d_more_filters\":\n",
    "        model = cnn3d_segformer_more_filters(cfg)\n",
    "    if model_arch == \"unet3d\":\n",
    "        model = unet3d_segformer(cfg)\n",
    "    if model_arch == \"unet3d_jumbo\":\n",
    "        model = unet3d_segformer_jumbo(cfg)\n",
    "    if model_arch == \"unetr\":\n",
    "        model = UNETR_Segformer(cfg)\n",
    "    if model_arch == \"unetr_mc\":\n",
    "        model = UNETR_SegformerMC(cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f168c01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.337829Z",
     "iopub.status.busy": "2023-06-14T16:56:01.336745Z",
     "iopub.status.idle": "2023-06-14T16:56:01.353471Z",
     "shell.execute_reply": "2023-06-14T16:56:01.352415Z"
    },
    "papermill": {
     "duration": 0.033329,
     "end_time": "2023-06-14T16:56:01.356342",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.323013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, use_tta=False):\n",
    "        self.models = []\n",
    "        self.use_tta = use_tta\n",
    "    def tta_infer(self, model:nn.Module, x):\n",
    "        #x.shape=(batch,c,h,w)\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=[model(single_x) for single_x in x]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=x.reshape(4,shape[0],*shape[3:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        if self.use_tta:\n",
    "            outputs = [self.tta_infer(model, x).to('cpu').numpy()\n",
    "                   for model in self.models]\n",
    "        else:\n",
    "            outputs = [model(x).mean(axis = 1).to('cpu').numpy()\n",
    "                       for model in self.models]\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "def build_ensemble_model(model_path, model_arch):\n",
    "    model = EnsembleModel(use_tta = True)\n",
    "    _model = build_model(CFG, model_arch)\n",
    "    _model.to(device)\n",
    "    state = torch.load(model_path)\n",
    "    try:\n",
    "        _model.load_state_dict(state)\n",
    "    except:\n",
    "        _model = nn.DataParallel(_model)\n",
    "        _model.load_state_dict(state)\n",
    "    _model.eval()\n",
    "\n",
    "    model.add_model(_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e781a384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.382994Z",
     "iopub.status.busy": "2023-06-14T16:56:01.382575Z",
     "iopub.status.idle": "2023-06-14T16:56:01.407937Z",
     "shell.execute_reply": "2023-06-14T16:56:01.406663Z"
    },
    "papermill": {
     "duration": 0.041571,
     "end_time": "2023-06-14T16:56:01.410668",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.369097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == 'test':\n",
    "    fragment_ids = sorted(os.listdir(CFG.comp_dataset_path + mode))\n",
    "else:\n",
    "    fragment_ids = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7bf600f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.437997Z",
     "iopub.status.busy": "2023-06-14T16:56:01.437565Z",
     "iopub.status.idle": "2023-06-14T16:56:01.455070Z",
     "shell.execute_reply": "2023-06-14T16:56:01.453650Z"
    },
    "papermill": {
     "duration": 0.035023,
     "end_time": "2023-06-14T16:56:01.457896",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.422873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tuples = [\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_3dcnn_segformer_best.pth\", \"segformer_config\": cnn_3d_config, \"score\": .75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_1024_3d_unet_segformer_final_all_train.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_512_3d_unet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_full_train_3dcnn_segformer_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_swa_slow_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\".74},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_3dunet_segformer_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_all_train_3dunet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_10_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_15_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_20_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_25_final.pth\", \"segformer_config\": cnn_3d_config},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 5,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b1_3dcnn_segformer_b1_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b1_config, \"score\":.71},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b2_3dcnn_segformer_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config, \"score\":.68},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_b4_3dcnn_segformer_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_bigsegformer_3dcnn_bigsegformer_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_10_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_30_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\": .74},\n",
    "    {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_all_train_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_all_train_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_512_b2_all_train_3dcnn_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config},\n",
    "    # ran at wrong resolution. Scored .73\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa_all_train.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.79},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_new_label_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_5_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\": .69},\n",
    "#     {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_1245_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config},\n",
    "#     {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_512_b5_unet_final_4Ryan.pth\", \"segformer_config\": unetr_multiclass_config, \"score\":.77},\n",
    "    {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_888_final_swa_all_train_long.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.82},\n",
    "    {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_NOVALIDATION_512_b5_unet_final_swa_all_train.pth\", \"segformer_config\": unetr_multiclass_config},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baedd7",
   "metadata": {
    "papermill": {
     "duration": 0.012363,
     "end_time": "2023-06-14T16:56:01.482977",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.470614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8128a58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.509903Z",
     "iopub.status.busy": "2023-06-14T16:56:01.509503Z",
     "iopub.status.idle": "2023-06-14T16:56:01.517432Z",
     "shell.execute_reply": "2023-06-14T16:56:01.516223Z"
    },
    "papermill": {
     "duration": 0.02438,
     "end_time": "2023-06-14T16:56:01.520106",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.495726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size = 20000):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros_like(probability, np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd8a630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.548882Z",
     "iopub.status.busy": "2023-06-14T16:56:01.547478Z",
     "iopub.status.idle": "2023-06-14T16:56:01.553530Z",
     "shell.execute_reply": "2023-06-14T16:56:01.552414Z"
    },
    "papermill": {
     "duration": 0.022776,
     "end_time": "2023-06-14T16:56:01.555928",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.533152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec000a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.582573Z",
     "iopub.status.busy": "2023-06-14T16:56:01.582187Z",
     "iopub.status.idle": "2023-06-14T16:56:01.607060Z",
     "shell.execute_reply": "2023-06-14T16:56:01.605799Z"
    },
    "papermill": {
     "duration": 0.041126,
     "end_time": "2023-06-14T16:56:01.609633",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.568507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    for fragment_id in fragment_ids:\n",
    "        mask_preds = None\n",
    "        last_res = None\n",
    "        for model_config in model_tuples:\n",
    "            mask_pred = None\n",
    "            mask_count = None\n",
    "            if last_res != model_config[\"size\"]:\n",
    "                for file in glob.glob(\"*.npy\"):\n",
    "                    os.remove(file)\n",
    "                last_res = model_config[\"size\"]\n",
    "            CFG.tile_size = model_config[\"tile_size\"]\n",
    "            CFG.size = model_config[\"size\"]\n",
    "            CFG.batch_size = model_config[\"batch_size\"]\n",
    "            CFG.stride = CFG.tile_size // 4\n",
    "            CFG.valid_aug_list = [\n",
    "                A.Resize(CFG.size, CFG.size),\n",
    "                A.Normalize(\n",
    "                    mean= [0] * CFG.in_chans,\n",
    "                    std= [1] * CFG.in_chans\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "            CFG.segformer_config = model_config[\"segformer_config\"]\n",
    "            model = build_ensemble_model(model_config[\"weight_path\"], model_config[\"model_arch\"])\n",
    "            test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "\n",
    "            binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/mask.png\", 0)\n",
    "            binary_mask = (binary_mask / 255).astype(int)\n",
    "\n",
    "            ori_h = binary_mask.shape[0]\n",
    "            ori_w = binary_mask.shape[1]\n",
    "            # mask = mask / 255\n",
    "\n",
    "            pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n",
    "            pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n",
    "\n",
    "            binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "            if mask_pred is None:\n",
    "                mask_pred = np.zeros(binary_mask.shape)\n",
    "                mask_count = np.zeros(binary_mask.shape)\n",
    "\n",
    "            for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "                images = images.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                with autocast():            \n",
    "                    with torch.no_grad():\n",
    "                        y_preds = model(images)\n",
    "\n",
    "                start_idx = step*CFG.batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "                    mask_pred[y1:y2, x1:x2] += y_preds[i]\n",
    "                    mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n",
    "            del test_loader\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "            mask_count = mask_count[:ori_h, :ori_w]\n",
    "            binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "\n",
    "            print(f'mask_count_min: {mask_count.min()}')\n",
    "            mask_pred = mask_pred/mask_count\n",
    "            mask_pred = torch.sigmoid(torch.tensor(mask_pred)).numpy()\n",
    "            if mask_preds is None:\n",
    "                mask_preds = mask_pred/len(model_tuples)\n",
    "            else:\n",
    "                mask_preds += mask_pred/len(model_tuples)\n",
    "\n",
    "        mask_pred = (mask_preds >= TH).astype(int)\n",
    "        mask_pred *= binary_mask\n",
    "        mask_pred = post_process(mask_pred.astype(float), TH, 10000).astype(int)\n",
    "        plt.imshow(mask_pred)\n",
    "        inklabels_rle = rle(mask_pred)\n",
    "        results.append((fragment_id, inklabels_rle))\n",
    "        del mask_pred, mask_count\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for file in glob.glob(\"*.npy\"):\n",
    "            os.remove(file)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142d46b",
   "metadata": {
    "papermill": {
     "duration": 0.012421,
     "end_time": "2023-06-14T16:56:01.634839",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.622418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "503e5f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.662053Z",
     "iopub.status.busy": "2023-06-14T16:56:01.661624Z",
     "iopub.status.idle": "2023-06-14T16:56:01.673327Z",
     "shell.execute_reply": "2023-06-14T16:56:01.672237Z"
    },
    "papermill": {
     "duration": 0.02818,
     "end_time": "2023-06-14T16:56:01.676031",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.647851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(results, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c369850b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.702335Z",
     "iopub.status.busy": "2023-06-14T16:56:01.701688Z",
     "iopub.status.idle": "2023-06-14T16:56:01.718810Z",
     "shell.execute_reply": "2023-06-14T16:56:01.717681Z"
    },
    "papermill": {
     "duration": 0.033011,
     "end_time": "2023-06-14T16:56:01.721272",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.688261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Predicted]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e3da14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.748670Z",
     "iopub.status.busy": "2023-06-14T16:56:01.747527Z",
     "iopub.status.idle": "2023-06-14T16:56:01.778950Z",
     "shell.execute_reply": "2023-06-14T16:56:01.777668Z"
    },
    "papermill": {
     "duration": 0.04794,
     "end_time": "2023-06-14T16:56:01.781769",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.733829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(CFG.comp_dataset_path + 'sample_submission.csv')\n",
    "sample_sub = pd.merge(sample_sub[['Id']], sub, on='Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1cb6bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.808627Z",
     "iopub.status.busy": "2023-06-14T16:56:01.808233Z",
     "iopub.status.idle": "2023-06-14T16:56:01.819978Z",
     "shell.execute_reply": "2023-06-14T16:56:01.818791Z"
    },
    "papermill": {
     "duration": 0.028162,
     "end_time": "2023-06-14T16:56:01.822408",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.794246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Id Predicted\n",
       "0  a       NaN\n",
       "1  b       NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "971b5fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T16:56:01.849823Z",
     "iopub.status.busy": "2023-06-14T16:56:01.848694Z",
     "iopub.status.idle": "2023-06-14T16:56:01.857974Z",
     "shell.execute_reply": "2023-06-14T16:56:01.856485Z"
    },
    "papermill": {
     "duration": 0.025832,
     "end_time": "2023-06-14T16:56:01.860663",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.834831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 104.261062,
   "end_time": "2023-06-14T16:56:04.777148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T16:54:20.516086",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
