{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3090fc-da18-4a47-ba96-3e967557d165",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "\n",
    "Use this notebook to run inference using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2fc9a3-6ebb-487e-a7ce-b9724fec373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (4.8.0.74)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from opencv-python) (1.25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfdd6a7-d23a-4cb2-94b8-d8c90d87bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (0.15.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (1.28.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: pathtools in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from wandb) (4.21.12)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045cb44f-68eb-43f7-b3c2-dc2e2cc22f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: torch>=1.9 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from monai) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from monai) (1.25.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (from torch>=1.9->monai) (4.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9755d90-5b33-4d3a-97ae-bd89f2ede704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5041d1d0",
   "metadata": {
    "papermill": {
     "duration": 4.628791,
     "end_time": "2023-06-14T16:54:36.924059",
     "exception": false,
     "start_time": "2023-06-14T16:54:32.295268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099916b5",
   "metadata": {
    "papermill": {
     "duration": 67.942023,
     "end_time": "2023-06-14T16:55:44.878300",
     "exception": false,
     "start_time": "2023-06-14T16:54:36.936277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/einops/einops-0.6.1-py3-none-any.whl\n",
    "# !pip install /kaggle/input/monai-packages/monai-1.1.0-202212191849-py3-none-any.whl[\"einops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7de1c-cc29-4108-aa2e-392c83bc33a5",
   "metadata": {},
   "source": [
    "To get the folders necessary for the path appends below, check the README for instructions.\n",
    "\n",
    "See the part that says  \n",
    "`!cd /root && git clone git@github.com:wolny/pytorch-3dunet.git && cd pytorch-3dunet && pip install -e .` You just need to grab the `get_model` function from this repo and grab the model directly.\n",
    "\n",
    "In this inference notebook we originally manually added the paths to `segmentation-models-pytorch`, `timm-pytorch-image-models`, `efficientnet-pytorch`, and `pretrainedmodels`, as well as the 3d-unet. On the actual notebook we submitted, we put the models on the github links above into the kaggle datasets below:\n",
    "\n",
    "https://www.kaggle.com/datasets/ryches/einops  \n",
    "https://www.kaggle.com/datasets/ryches/3d-unet  \n",
    "https://www.kaggle.com/datasets/ryches/unet3d  \n",
    "\n",
    "Here are the links to the other repos. You likely won't have to install these manually as they're included in `requirements.txt`. We're including links anyway in case you have to go that route. Make sure to edit the `sys.path.append` calls below to include if you're grabbing these manually (if you can import as-is, feel free to comment out the `sys.path.append`s)\n",
    "\n",
    "https://github.com/qubvel/segmentation_models.pytorch  \n",
    "https://timm.fast.ai/  \n",
    "https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "The `pretrainedmodels` folder contains the weights we created during training.\n",
    "You might have to do a bit of manual massaging to get all the imports you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb75315",
   "metadata": {
    "papermill": {
     "duration": 15.172294,
     "end_time": "2023-06-14T16:56:00.062639",
     "exception": false,
     "start_time": "2023-06-14T16:55:44.890345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('externals')\n",
    "\n",
    "\n",
    "# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n",
    "# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n",
    "# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n",
    "# sys.path.append('/kaggle/input/unet3d/pytorch3dunet/pytorch3dunet')\n",
    "# sys.path.append('/kaggle/input/unet3d/pytorch3dunet')\n",
    "# sys.path.append('/kaggle/input/unet3d/')\n",
    "\n",
    "import segmentation_models_pytorch as smpx\n",
    "from pytorch3dunet.unet3d.model import get_model\n",
    "from unetr import UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eebbc05",
   "metadata": {
    "papermill": {
     "duration": 0.412184,
     "end_time": "2023-06-14T16:56:00.487302",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.075118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345fd4",
   "metadata": {
    "papermill": {
     "duration": 0.0123,
     "end_time": "2023-06-14T16:56:00.512292",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.499992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40f8bc1",
   "metadata": {
    "papermill": {
     "duration": 0.035624,
     "end_time": "2023-06-14T16:56:00.561034",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.525410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "\n",
    "    # comp_dir_path = './'\n",
    "    comp_dir_path = '/kaggle/input/'\n",
    "    comp_folder_name = 'vesuvius-challenge-ink-detection'\n",
    "    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n",
    "    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n",
    "    \n",
    "    exp_name = '3d_unet_subv2'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 1\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = '3d_unet_segformer'\n",
    "    backbone = 'None'\n",
    "#     backbone = 'se_resnext50_32x4d'\n",
    "\n",
    "    in_chans = 16\n",
    "    # ============== training cfg =============\n",
    "    size = 1024\n",
    "    tile_size = 1024\n",
    "    stride = tile_size // 4\n",
    "\n",
    "    batch_size = 3 # 32\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 15\n",
    "\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4 / warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    valid_id = 2\n",
    "\n",
    "    objective_cv = 'binary'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'maximize'  # maximize, 'minimize'\n",
    "    # metrics = 'dice_coef'\n",
    "\n",
    "    # ============== fixed =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'best'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 50\n",
    "    num_workers = 2\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07166ff2",
   "metadata": {
    "papermill": {
     "duration": 0.023514,
     "end_time": "2023-06-14T16:56:00.597217",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.573703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_DEBUG = False\n",
    "mode = 'train' if IS_DEBUG else 'test'\n",
    "TH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc296dd",
   "metadata": {
    "papermill": {
     "duration": 0.090446,
     "end_time": "2023-06-14T16:56:00.700677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.610231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fcc0c",
   "metadata": {
    "papermill": {
     "duration": 0.012051,
     "end_time": "2023-06-14T16:56:00.725379",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.713328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d3181a",
   "metadata": {
    "papermill": {
     "duration": 0.024568,
     "end_time": "2023-06-14T16:56:00.762072",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.737504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba57cd",
   "metadata": {
    "papermill": {
     "duration": 0.012549,
     "end_time": "2023-06-14T16:56:00.787362",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.774813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9caa21a8",
   "metadata": {
    "papermill": {
     "duration": 0.027887,
     "end_time": "2023-06-14T16:56:00.827677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.799790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "#     idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.in_chans // 2\n",
    "    end = mid + CFG.in_chans // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f54b80e",
   "metadata": {
    "papermill": {
     "duration": 0.027028,
     "end_time": "2023-06-14T16:56:00.866823",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.839795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = np.array(images)\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.images[idx])\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image[None, :, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f476714",
   "metadata": {
    "papermill": {
     "duration": 0.027652,
     "end_time": "2023-06-14T16:56:00.906428",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.878776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in y1_list:\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.tile_size\n",
    "            x2 = x1 + CFG.tile_size\n",
    "            if test_images[y1:y2, x1:x2].max() != 0:\n",
    "                if not os.path.exists(f\"{x1}_{y1}_{x2}_{y2}.npy\"):\n",
    "                    np.save(f\"{x1}_{y1}_{x2}_{y2}.npy\", test_images[y1:y2, x1:x2])\n",
    "                test_images_list.append(f\"{x1}_{y1}_{x2}_{y2}.npy\")\n",
    "                xyxys.append((x1, y1, x2, y2))\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8db65",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2023-06-14T16:56:00.930782",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.918816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26ff61-0019-4504-bbf0-3c270908e9c1",
   "metadata": {},
   "source": [
    "Below we define the configs for different models we used in the eventual ensemble. We changed which models we actually included depending on the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043fad1b",
   "metadata": {
    "papermill": {
     "duration": 0.026857,
     "end_time": "2023-06-14T16:56:00.970244",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.943387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerModel, SegformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a31cc5",
   "metadata": {
    "papermill": {
     "duration": 0.027797,
     "end_time": "2023-06-14T16:56:01.009859",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.982062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b1_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 256,\n",
    "  \"depths\": [\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e6c1c5",
   "metadata": {
    "papermill": {
     "duration": 0.027332,
     "end_time": "2023-06-14T16:56:01.049571",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.022239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b2_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    6,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8372d5ee",
   "metadata": {
    "papermill": {
     "duration": 0.027397,
     "end_time": "2023-06-14T16:56:01.092770",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.065373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b4_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    8,\n",
    "    27,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108de382",
   "metadata": {
    "papermill": {
     "duration": 0.027203,
     "end_time": "2023-06-14T16:56:01.133534",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.106331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b5_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b90b4da",
   "metadata": {
    "papermill": {
     "duration": 0.038961,
     "end_time": "2023-06-14T16:56:01.184541",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.145580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":32})\n",
    "cnn_3d_more_filters_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":64})\n",
    "\n",
    "unet_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 3,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561503be",
   "metadata": {
    "papermill": {
     "duration": 0.033891,
     "end_time": "2023-06-14T16:56:01.230741",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.196850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_3d_jumbo_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})\n",
    "\n",
    "unetr_multiclass_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95594b7c",
   "metadata": {
    "papermill": {
     "duration": 0.066905,
     "end_time": "2023-06-14T16:56:01.310516",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.243611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unetr import UNETR\n",
    "from externals.models import cnn3d_segformer, cnn3d_segformer_more_filters, unet3d_segformer, unet3d_segformer_jumbo, UNETR_Segformer, UNETR_SegformerMC\n",
    "# from externals.models import CNN3D_Segformer, Unet3D_Segformer, CNN3D_Unet, CNN3D_MANet, CNN3D_EfficientUnetplusplusb5, CNN3D_SegformerB4\n",
    "\n",
    "def build_model(cfg, model_arch = None):\n",
    "    print('model_name', cfg.model_name)\n",
    "    if model_arch == \"cnn3d\":\n",
    "        model = cnn3d_segformer(cfg)\n",
    "    if model_arch == \"cnn3d_more_filters\":\n",
    "        model = cnn3d_segformer_more_filters(cfg)\n",
    "    if model_arch == \"unet3d\":\n",
    "        model = unet3d_segformer(cfg)\n",
    "    if model_arch == \"unet3d_jumbo\":\n",
    "        model = unet3d_segformer_jumbo(cfg)\n",
    "    if model_arch == \"unetr\":\n",
    "        model = UNETR_Segformer(cfg)\n",
    "    if model_arch == \"unetr_mc\":\n",
    "        model = UNETR_SegformerMC(cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f168c01f",
   "metadata": {
    "papermill": {
     "duration": 0.033329,
     "end_time": "2023-06-14T16:56:01.356342",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.323013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, use_tta=False):\n",
    "        self.models = []\n",
    "        self.use_tta = use_tta\n",
    "    def tta_infer(self, model:nn.Module, x):\n",
    "        #x.shape=(batch,c,h,w)\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=[model(single_x) for single_x in x]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=x.reshape(4,shape[0],*shape[3:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        if self.use_tta:\n",
    "            outputs = [self.tta_infer(model, x).to('cpu').numpy()\n",
    "                   for model in self.models]\n",
    "        else:\n",
    "            outputs = [model(x).mean(axis = 1).to('cpu').numpy()\n",
    "                       for model in self.models]\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "def build_ensemble_model(model_path, model_arch):\n",
    "    model = EnsembleModel(use_tta = True)\n",
    "    _model = build_model(CFG, model_arch)\n",
    "    _model.to(device)\n",
    "    state = torch.load(model_path)\n",
    "    try:\n",
    "        _model.load_state_dict(state)\n",
    "    except:\n",
    "        _model = nn.DataParallel(_model)\n",
    "        _model.load_state_dict(state)\n",
    "    _model.eval()\n",
    "\n",
    "    model.add_model(_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03fb4b-87c7-416e-b214-3234f64587eb",
   "metadata": {},
   "source": [
    "Make sure you have the folders with fragment IDs added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e781a384",
   "metadata": {
    "papermill": {
     "duration": 0.041571,
     "end_time": "2023-06-14T16:56:01.410668",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.369097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/vesuvius-challenge-ink-detection/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     fragment_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomp_dataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     fragment_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/vesuvius-challenge-ink-detection/test'"
     ]
    }
   ],
   "source": [
    "if mode == 'test':\n",
    "    fragment_ids = sorted(os.listdir(CFG.comp_dataset_path + mode))\n",
    "else:\n",
    "    fragment_ids = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89428df8-d638-4c9a-93c6-d7992c03c2b4",
   "metadata": {},
   "source": [
    "## Select models you want to use here\n",
    "\n",
    "We just commented/uncommented this list for models we trained when we did inference. You'll need pretrained weights / pretrained models here from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7bf600f",
   "metadata": {
    "papermill": {
     "duration": 0.035023,
     "end_time": "2023-06-14T16:56:01.457896",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.422873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tuples = [\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_3dcnn_segformer_best.pth\", \"segformer_config\": cnn_3d_config, \"score\": .75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_1024_3d_unet_segformer_final_all_train.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_512_3d_unet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_full_train_3dcnn_segformer_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_swa_slow_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\".74},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_3dunet_segformer_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_all_train_3dunet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_10_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_15_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_20_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_25_final.pth\", \"segformer_config\": cnn_3d_config},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 5,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b1_3dcnn_segformer_b1_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b1_config, \"score\":.71},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b2_3dcnn_segformer_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config, \"score\":.68},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_b4_3dcnn_segformer_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_bigsegformer_3dcnn_bigsegformer_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_10_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_30_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\": .74},\n",
    "    {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_all_train_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_all_train_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_512_b2_all_train_3dcnn_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config},\n",
    "    # ran at wrong resolution. Scored .73\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa_all_train.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.75},\n",
    "    {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.79},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_new_label_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_5_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\": .69},\n",
    "#     {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_1245_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config},\n",
    "#     {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_512_b5_unet_final_4Ryan.pth\", \"segformer_config\": unetr_multiclass_config, \"score\":.77},\n",
    "    {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_888_final_swa_all_train_long.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.82},\n",
    "    {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_NOVALIDATION_512_b5_unet_final_swa_all_train.pth\", \"segformer_config\": unetr_multiclass_config},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baedd7",
   "metadata": {
    "papermill": {
     "duration": 0.012363,
     "end_time": "2023-06-14T16:56:01.482977",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.470614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8128a58d",
   "metadata": {
    "papermill": {
     "duration": 0.02438,
     "end_time": "2023-06-14T16:56:01.520106",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.495726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size = 20000):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros_like(probability, np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdd8a630",
   "metadata": {
    "papermill": {
     "duration": 0.022776,
     "end_time": "2023-06-14T16:56:01.555928",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.533152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec000a10",
   "metadata": {
    "papermill": {
     "duration": 0.041126,
     "end_time": "2023-06-14T16:56:01.609633",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.568507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    for fragment_id in fragment_ids:\n",
    "        mask_preds = None\n",
    "        last_res = None\n",
    "        for model_config in model_tuples:\n",
    "            mask_pred = None\n",
    "            mask_count = None\n",
    "            if last_res != model_config[\"size\"]:\n",
    "                for file in glob.glob(\"*.npy\"):\n",
    "                    os.remove(file)\n",
    "                last_res = model_config[\"size\"]\n",
    "            CFG.tile_size = model_config[\"tile_size\"]\n",
    "            CFG.size = model_config[\"size\"]\n",
    "            CFG.batch_size = model_config[\"batch_size\"]\n",
    "            CFG.stride = CFG.tile_size // 4\n",
    "            CFG.valid_aug_list = [\n",
    "                A.Resize(CFG.size, CFG.size),\n",
    "                A.Normalize(\n",
    "                    mean= [0] * CFG.in_chans,\n",
    "                    std= [1] * CFG.in_chans\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "            CFG.segformer_config = model_config[\"segformer_config\"]\n",
    "            model = build_ensemble_model(model_config[\"weight_path\"], model_config[\"model_arch\"])\n",
    "            test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "\n",
    "            binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/mask.png\", 0)\n",
    "            binary_mask = (binary_mask / 255).astype(int)\n",
    "\n",
    "            ori_h = binary_mask.shape[0]\n",
    "            ori_w = binary_mask.shape[1]\n",
    "            # mask = mask / 255\n",
    "\n",
    "            pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n",
    "            pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n",
    "\n",
    "            binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "            if mask_pred is None:\n",
    "                mask_pred = np.zeros(binary_mask.shape)\n",
    "                mask_count = np.zeros(binary_mask.shape)\n",
    "\n",
    "            for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "                images = images.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                with autocast():            \n",
    "                    with torch.no_grad():\n",
    "                        y_preds = model(images)\n",
    "\n",
    "                start_idx = step*CFG.batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "                    mask_pred[y1:y2, x1:x2] += y_preds[i]\n",
    "                    mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n",
    "            del test_loader\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "            mask_count = mask_count[:ori_h, :ori_w]\n",
    "            binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "\n",
    "            print(f'mask_count_min: {mask_count.min()}')\n",
    "            mask_pred = mask_pred/mask_count\n",
    "            mask_pred = torch.sigmoid(torch.tensor(mask_pred)).numpy()\n",
    "            if mask_preds is None:\n",
    "                mask_preds = mask_pred/len(model_tuples)\n",
    "            else:\n",
    "                mask_preds += mask_pred/len(model_tuples)\n",
    "\n",
    "        mask_pred = (mask_preds >= TH).astype(int)\n",
    "        mask_pred *= binary_mask\n",
    "        mask_pred = post_process(mask_pred.astype(float), TH, 10000).astype(int)\n",
    "        plt.imshow(mask_pred)\n",
    "        inklabels_rle = rle(mask_pred)\n",
    "        results.append((fragment_id, inklabels_rle))\n",
    "        del mask_pred, mask_count\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for file in glob.glob(\"*.npy\"):\n",
    "            os.remove(file)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142d46b",
   "metadata": {
    "papermill": {
     "duration": 0.012421,
     "end_time": "2023-06-14T16:56:01.634839",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.622418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e5f46",
   "metadata": {
    "papermill": {
     "duration": 0.02818,
     "end_time": "2023-06-14T16:56:01.676031",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.647851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(results, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369850b",
   "metadata": {
    "papermill": {
     "duration": 0.033011,
     "end_time": "2023-06-14T16:56:01.721272",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.688261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3da14f",
   "metadata": {
    "papermill": {
     "duration": 0.04794,
     "end_time": "2023-06-14T16:56:01.781769",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.733829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(CFG.comp_dataset_path + 'sample_submission.csv')\n",
    "sample_sub = pd.merge(sample_sub[['Id']], sub, on='Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb6bd7",
   "metadata": {
    "papermill": {
     "duration": 0.028162,
     "end_time": "2023-06-14T16:56:01.822408",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.794246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b5fb3",
   "metadata": {
    "papermill": {
     "duration": 0.025832,
     "end_time": "2023-06-14T16:56:01.860663",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.834831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 104.261062,
   "end_time": "2023-06-14T16:56:04.777148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T16:54:20.516086",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
